#+title: Readme
#+subtitle: Bridge for Circuit's Non-standard OpenAPI Integration
#+author: David Connett

* Overview

Circuit's API differs from the standard OpenAPI calling conventions commonly expected by developer tools and libraries:

- Models are selected using a *deployment ID in the URL path*, not a ~model~ parameter.
- API authentication uses *OAuth2 tokens* (which expire hourly), not static API keys.
- Integration tools often *expect standard, parameter-based* OpenAPI flows.

This project provides a minimal local webserver that acts as a translation layer between *standard OpenAPI clients* and *Circuit's API*.

* Features

- Exposes a standard OpenAPI-compatible endpoint (e.g., ~POST /v1/chat/completions~).
- Accepts requests with a ~model~ parameter and static API key (compatible with typical OpenAPI tools).
- Converts requests to Circuit's required format:
  - Moves ~model~ parameter into the deployment path segment.
  - Fetches and caches Circuit OAuth2 tokens for use as the API key.
- Handles token refresh transparently; clients continue to use a static API key.
- Optional *HTTPS* support with self-signed certificates (dual-mode HTTP/HTTPS).
- Health endpoint ~GET /health~ for connectivity checks.
- Verbose request/response logging for easy debugging (colorized; process-tagged for HTTP/HTTPS in dual mode).
- Can integrate with tools such as OpenAI SDKs, Ollama, LangChain, LlamaIndex, etc.

* Why?

- *Portability*: Use standard tools for LLM access with Circuit's infrastructure.
- *Simplicity*: Remove the need for complex, recurring OAuth2 token generation in every client.
- *Interoperability*: Bridge gap between CLI tools, libraries, and Circuit's deployment-based flow.

* Quick Start

See [[file:QUICKSTART.org][QUICKSTART.org]] for a step-by-step setup including certificate generation, starting the server in HTTP/HTTPS modes, and example requests.

* Usage

1. Start the webserver locally.
2. Point your tool's OpenAPI endpoint and API key config to the local server.
3. The server will translate and forward your requests to Circuit.

HTTP example ::

#+begin_src sh
curl http://localhost:12000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
#+end_src

HTTPS example (self-signed cert) ::

#+begin_src sh
curl -k https://localhost:12443/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
#+end_src

The server:
- Rewrites the request to ~https://chat-ai.cisco.com/openai/deployments/<model>/chat/completions~
- Inserts the correct OAuth2 token

* Configuration

- Provide Circuit Okta credentials and an appkey (see [[file:circuit_api.org][circuit_api.org]]):
  - ~CIRCUIT_CLIENT_ID~
  - ~CIRCUIT_CLIENT_SECRET~
  - ~CIRCUIT_APPKEY~
- (Optional) Generate a development certificate:
  #+begin_src sh
  python generate_cert.py
  #+end_src
- Start the server:
  - HTTP only: ~python rewriter.py~
  - HTTPS only: ~python rewriter.py --ssl-only~
  - Dual mode (HTTP + HTTPS): ~python rewriter.py --ssl~

* ChatGPT CLI (demo)

- Project: [[https://github.com/kardolus/chatgpt-cli][kardolus/chatgpt-cli]]
- Use the provided config file to point the CLI at the local bridge:
  #+begin_src sh
  chatgpt --config /Users/daconnet/Git/oai-to-circuit/chatgpt-cli.openai.yaml "Say hello from the bridge"
  #+end_src
- Or export environment variables (override config):
  #+begin_src sh
  export OPENAI_URL=http://localhost:12000
  export OPENAI_COMPLETIONS_PATH=/v1/chat/completions
  export OPENAI_API_KEY=demo_key # This is ignored, and is arbitrary.
  export OPENAI_MODEL=gpt-4o-mini
  chatgpt "Say hello from the bridge"
  #+end_src

* Python SDK Demo

- A runnable Python example using the OpenAI SDK is included:
  #+begin_src sh
  python python_openai_demo.py --prompt "Say hello from the bridge"
  #+end_src
- HTTPS with self-signed cert:
  #+begin_src sh
  python python_openai_demo.py --https --insecure-skip-verify --prompt "Say hello over HTTPS"
  #+end_src
- Streaming tokens:
  #+begin_src sh
  python python_openai_demo.py --stream --prompt "Count to 5"
  #+end_src

* Health Check

- ~GET /health~ returns a simple JSON indicating service status and configuration flags.

* Debugging

- Common error: ~Invalid HTTP request received~
  - Often caused by sending *HTTPS* to the *HTTP* endpoint.
  - Run the diagnostic script:
    #+begin_src sh
    python debug_invalid_http.py
    #+end_src
- Server logs include detailed request headers and bodies (when safe) to aid debugging.

* Tests & Examples

Run the full test suite:
#+begin_src sh
pytest
#+end_src

Individual test modules in ~tests/~:
- ~test_quota.py~ – quota enforcement and usage tracking
- ~test_requests.py~ – endpoint behavior and error scenarios
- ~test_subkey_extraction.py~ – header parsing
- ~test_oauth.py~ – token caching and authentication
- ~test_config.py~ – configuration loading
- ~test_https.py~ – SSL configuration validation

Usage examples:
- ~examples.py~ – curl and SDK examples
- ~python_openai_demo.py~ – Python OpenAI SDK integration
- ~debug_invalid_http.py~ – connection diagnostics

* Roadmap

- [X] Basic translation for chat completions
- [X] Automated and cached OAuth2 token management
- [X] Optional HTTPS support (self-signed, dual-mode)
- [ ] Support for additional OpenAPI methods/endpoints as needed

* References

- [[https://ai-chat.cisco.com/bridgeit-platform/api/home][Cisco CIRCUIT Chat API]]
- See ~circuit_api.org~ for specifics on the underlying API flow.

* Author

David Connett
